{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you approach resolving a moral dilemma involving a self-driving car faced with a situation where it must choose between harming one person or multiple pedestrians in a split-second decision?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Resolving a moral dilemma involving a self-driving car, particularly one that must make split-second decisions about whom to harm, is complex and involves various ethical considerations. Here’s an approach to analyze the situation:\n",
       "\n",
       "1. **Framework Selection**: Identify the ethical frameworks that could guide the decision-making. Two common frameworks are:\n",
       "   - **Utilitarianism**: This perspective focuses on maximizing overall happiness or minimizing harm, suggesting that the car should aim to harm the fewest number of people.\n",
       "   - **Deontological Ethics**: This perspective emphasizes the importance of rules and duties, which might suggest that causing harm to any individual is inherently wrong, regardless of the consequences.\n",
       "\n",
       "2. **Value Assessment**: Consider the value of the lives at stake. Factors might include age, potential for future contributions to society, or contextual roles (e.g., a pedestrian crossing the street versus a person in a car who may have chosen to engage in risky behavior).\n",
       "\n",
       "3. **Transparency and Accountability**: Ensure that the algorithms guiding the car's decisions are transparent and that stakeholders (manufacturers, policymakers, users) are involved in establishing the moral guidelines. Public input on acceptable harm thresholds can help guide programming.\n",
       "\n",
       "4. **Regulations and Standards**: Collaborate with ethicists, policy-makers, and legal experts to establish clear guidelines for how self-driving cars should be programmed to handle such dilemmas. This could involve creating standards that dictate life preservation strategies.\n",
       "\n",
       "5. **Real-time Data and Predictive Algorithms**: Investigate how the car can use real-time data to make the decision process more accurate. For instance, if it can assess the severity of the potential impact, it could make an informed decision rather than a purely mathematical one.\n",
       "\n",
       "6. **Testing and Simulation**: Conduct simulations to explore various scenarios and outcomes. These tests can help refine the decision-making algorithms and assess public reaction to different decision-making frameworks.\n",
       "\n",
       "7. **Balancing Innovation and Ethics**: Recognize that while the technology is evolving rapidly, ethical considerations should keep pace with innovation. Ongoing discussions and evaluations are necessary to adapt to changing societal norms.\n",
       "\n",
       "8. **Public Engagement**: Encourage ongoing dialogue with the public about their values and preferences regarding self-driving cars. This could help guide ethical considerations and inform societal norms around the use of such technologies.\n",
       "\n",
       "Ultimately, it is important to acknowledge that no matter the approach, the decision made by a self-driving car will inevitably reflect human values and societal norms, making the involvement of diverse stakeholders crucial in grappling with these ethical dilemmas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "# model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "# claude = Anthropic()\n",
    "# response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "# answer = response.content[0].text\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is the classic \"trolley problem\" re-imagined for the age of autonomous vehicles, and it's a deeply complex moral dilemma with no easy answer. My approach would involve considering the following interconnected aspects:\n",
       "\n",
       "**1. Utilitarianism vs. Deontology:**\n",
       "\n",
       "*   **Utilitarianism (Greatest Good for the Greatest Number):** A purely utilitarian approach would prioritize minimizing harm, likely choosing to sacrifice the single person to save multiple pedestrians. This aligns with the idea of maximizing overall well-being. However, it can be seen as inherently unjust to the individual sacrificed.\n",
       "*   **Deontology (Moral Duty and Rules):** A deontological approach focuses on adherence to moral rules and principles, regardless of the consequences. This might mean prioritizing the safety of the car's occupants (assuming the single person is inside the car) or adhering to a principle like \"do no harm.\" However, this could result in the car knowingly causing more harm than necessary.\n",
       "\n",
       "**2. Pre-programmed Algorithms vs. Dynamic Decision-Making:**\n",
       "\n",
       "*   **Pre-programmed Algorithms:** Establishing fixed rules beforehand, based on different scenarios, is appealing for its predictability and transparency.  These rules could be based on utilitarian or deontological principles, or a hybrid of both. The challenge lies in anticipating all possible scenarios and defining the specific thresholds (e.g., number of pedestrians vs. occupant safety) that trigger different actions.  This requires public debate and ethical consensus.\n",
       "*   **Dynamic Decision-Making (AI-driven):** Allowing the car to dynamically analyze the situation and make a \"best\" choice in real-time could lead to more nuanced decisions. This could involve factors like the age and vulnerability of potential victims, the likelihood of survival in each scenario, and the potential long-term consequences. However, this approach raises concerns about accountability, bias in the AI algorithm, and the perceived lack of control.  Furthermore, explaining why the AI made a specific decision would be extremely difficult.\n",
       "\n",
       "**3. Transparency and Accountability:**\n",
       "\n",
       "*   **Public Debate and Ethical Frameworks:**  A broad public discussion involving ethicists, policymakers, engineers, and the public is crucial to establish ethical guidelines and legal frameworks for autonomous vehicle behavior in these situations. This includes defining the values that should guide these algorithms and establishing clear lines of responsibility in case of accidents.\n",
       "*   **Transparency in Algorithm Design:**  The principles and parameters of the algorithms that govern these decisions should be transparent and accessible to the public. This allows for scrutiny and potential improvements based on feedback and evolving societal values.  However, the precise implementation might need to remain proprietary for security reasons.\n",
       "*   **Post-Accident Analysis:**  Thorough investigations of accidents involving autonomous vehicles should be conducted to understand the reasoning behind the car's decisions and to identify areas for improvement in the algorithms and safety systems.\n",
       "\n",
       "**4. Prioritizing Accident Prevention:**\n",
       "\n",
       "*   **Investing in Safety Technology:**  The primary focus should be on developing and implementing technologies that prevent these dilemmas from arising in the first place. This includes advanced sensor systems, redundant braking systems, sophisticated collision avoidance algorithms, and improved road infrastructure.\n",
       "*   **Predictive Modeling and Risk Mitigation:**  Autonomous vehicle algorithms should incorporate predictive modeling to anticipate potential hazards and proactively adjust speed and trajectory to avoid dangerous situations.\n",
       "*   **Continuous Learning and Improvement:**  Autonomous vehicles should continuously learn from their experiences and update their algorithms to improve safety performance over time.\n",
       "\n",
       "**5. Legal and Regulatory Framework:**\n",
       "\n",
       "*   **Liability and Responsibility:**  Clear legal frameworks are needed to define liability in the event of an accident involving an autonomous vehicle. This includes determining who is responsible (e.g., the manufacturer, the owner, the programmer) and establishing mechanisms for compensation to victims.\n",
       "*   **Data Privacy and Security:**  Autonomous vehicles generate vast amounts of data, and it's crucial to ensure that this data is protected from unauthorized access and misuse.  Privacy policies must be transparent and comply with relevant regulations.\n",
       "\n",
       "**In Summary:**\n",
       "\n",
       "There is no perfect solution to this moral dilemma. My approach would emphasize a multi-faceted strategy that combines:\n",
       "\n",
       "*   **Ethical deliberation:** involving diverse stakeholders to establish guiding principles.\n",
       "*   **Technological innovation:** focusing on preventing accidents and minimizing harm.\n",
       "*   **Transparent and accountable algorithms:** allowing for public scrutiny and continuous improvement.\n",
       "*   **Robust legal and regulatory frameworks:** defining liability and ensuring data privacy.\n",
       "\n",
       "The goal is not to find a \"right\" answer, but to create a system that is as safe, fair, and transparent as possible, given the inherent limitations of technology and the complexity of human morality. It's an ongoing process that requires constant re-evaluation and adaptation as technology advances and societal values evolve.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The scenario you describe—where a government uses memory and perception manipulation technology to maintain societal stability—raises profound ethical questions that touch on autonomy, consent, utilitarianism, and the nature of a just society. Below is a structured analysis of the ethical implications, the balance of benefits and consequences, and how such a society might navigate these tensions.\n",
       "\n",
       "### **Ethical Implications**\n",
       "1. **Violation of Autonomy and Consent**  \n",
       "   - Altering memories without consent fundamentally undermines individual autonomy, a core principle of ethics (Kantian deontology argues that treating people as ends, not means, requires respecting their ability to make informed choices).  \n",
       "   - If citizens are unaware of the manipulation, they cannot meaningfully consent, making the practice inherently coercive.\n",
       "\n",
       "2. **Erosion of Authenticity and Identity**  \n",
       "   - Memories shape personal identity. Manipulating them could lead to a loss of self, where individuals no longer have a coherent sense of their past or agency in shaping their future.  \n",
       "   - A society built on false perceptions risks becoming a collective illusion, devoid of genuine human experience.\n",
       "\n",
       "3. **Potential for Abuse**  \n",
       "   - Such power is ripe for misuse—governments could erase dissent, fabricate loyalty, or justify oppression under the guise of \"stability.\"  \n",
       "   - Historical precedents (e.g., propaganda, censorship) show that even well-intentioned control often leads to authoritarianism.\n",
       "\n",
       "4. **Utilitarian Trade-offs**  \n",
       "   - If stability prevents violence or suffering, a utilitarian might argue the benefits (e.g., reduced crime, social harmony) justify the costs.  \n",
       "   - However, this risks justifying tyranny if \"greater good\" arguments override individual rights.\n",
       "\n",
       "### **Do Benefits Outweigh Consequences?**  \n",
       "**Potential Benefits:**  \n",
       "   - Elimination of traumatic memories that cause individual suffering.  \n",
       "   - Prevention of social unrest by erasing divisive ideologies or hatred.  \n",
       "   - Rapid resolution of conflicts by altering perceptions of past grievances.  \n",
       "\n",
       "**Potential Consequences:**  \n",
       "   - Loss of free will and moral agency.  \n",
       "   - A society built on lies may collapse when truth inevitably surfaces.  \n",
       "   - Stifling dissent could stagnate progress, as societal improvement often arises from confronting uncomfortable truths.  \n",
       "\n",
       "*On balance*, the long-term consequences (erosion of trust, identity, and freedom) likely outweigh short-term stability. A society that relies on manipulation risks becoming fragile and morally bankrupt.\n",
       "\n",
       "### **Balancing Collective Peace and Individual Rights**  \n",
       "If such technology exists, ethical use would require:  \n",
       "1. **Strict Limitations**  \n",
       "   - Only allow voluntary, consensual memory alteration (e.g., therapy for PTSD).  \n",
       "   - Independent oversight to prevent government overreach.  \n",
       "\n",
       "2. **Transparency and Accountability**  \n",
       "   - Public debate on when (if ever) such technology should be used.  \n",
       "   - Clear legal frameworks defining permissible uses (e.g., never for political control).  \n",
       "\n",
       "3. **Alternative Approaches to Stability**  \n",
       "   - Address root causes of instability (inequality, injustice) rather than masking symptoms.  \n",
       "   - Foster resilience through education and dialogue, not coercion.  \n",
       "\n",
       "### **Conclusion**  \n",
       "While memory manipulation might offer superficial stability, a just society cannot prioritize peace at the expense of truth and autonomy. The ethical path lies in empowering individuals—not controlling them—and addressing societal problems through democratic and transparent means. As philosopher John Stuart Mill argued, freedom is essential for human flourishing, even if it entails some disorder. A society that trades liberty for stability may end up with neither."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages, stream=False)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "# model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2\n",
    "\n",
    "# Exclamation mark commands are used to run shell commands directly from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, the user is asking about a classic trolley problem scenario but applied to a self-driving car. In this case, the car has to choose between two outcomes: causing harm to one person (e.g., by swerving and hitting that person) or harming multiple people (e.g., not swerving and allowing the pedestrian group to be hit).\n",
       "\n",
       "I need to approach this resolution without being too prescriptive because it's a moral dilemma. There are no easy answers, and different ethical frameworks can lead to different conclusions.\n",
       "\n",
       "Key elements from classical ethics: I should draw on utilitarianism, deontology, virtue ethics, etc., but keep it balanced. Also, consider the context—what defines a person or pedestrian? It could be adults vs. children, for example, which changes the stakes.\n",
       "\n",
       "Context matters because people aren't identical. But in this scenario, since both options involve harming living beings, I need to assume some default (like avoiding greater harm).\n",
       "\n",
       "The user said \"choose between harming one person or multiple pedestrians.\" It might imply that these are different types of victims—e.g., a known individual vs. strangers. In ethics, we often have problems with the distinction.\n",
       "\n",
       "Break down my approach:\n",
       "\n",
       "1. **Acknowledge the dilemma:** Explain that this is an ethical issue where absolute morality intersects with programming choices. Use frameworks like the trolley problem to show its parallels.\n",
       "\n",
       "2. **Explain key concepts in AI ethics for self-driving cars:**\n",
       "   - Intent vs. outcome: Some argue about intention (e.g., does it matter who we try to save?) versus net utility or harm reduction.\n",
       "   - Programming constraints: The car should minimize global risk, perhaps with pre-defined weights based on societal consensus.\n",
       "\n",
       "3. **Potential resolutions and ethical considerations:**\n",
       "   - Utilitarianism-based: Minimize overall harm; choose the option that saves more lives (e.g., hit one person if it results in fewer deaths or less severe outcomes).\n",
       "   - Deontology-based: Focus on rules, like not causing harm to anyone regardless of circumstance.\n",
       "   - Other heuristics: Age, size, identity. For instance, does knowing a passenger versus an outsider affect responsibility?\n",
       "     - Also, social equity aspects.\n",
       "\n",
       "4. **Practical steps for AI systems:** Since this is about self-driving cars, discuss how engineers might code such decisions or avoid them entirely.\n",
       "   - Value alignment: How society would \"align the value system\" of AI to handle these scenarios ethically without being reduced to binary algorithmic choices?\n",
       "   - Uncertainty and split-second decision requirements.\n",
       "\n",
       "Structure my response:\n",
       "- Introduction: Define the situation briefly.\n",
       "- Ethical frameworks: Discuss 1-2 key theories that could shed light on this.\n",
       "- Potential resolutions: Offer a reasoned approach, such as preferring fewer lives lost, but admit it's complex with factors like certain death versus potential harm etcetera—this is important because in reality there might be nuances I'm missing. For example, if swerving would definitely kill one person while not swerving could have ambiguous outcomes or partial damage to property only.\n",
       "- Caveats: Stress that this isn't the real decision since AIs don't have emotions; it's about programming based on values agreed upon by society and AI researchers.\n",
       "\n",
       "AI ethics resources: I can reference ethical principles like those from IEEE, UN Human Rights Council, etc., which are starting to guide such decisions. For instance, they emphasize safety over maximizing outcomes unless necessary.\n",
       "\n",
       "My response should be professional, empathetic, clear, and concise.\n",
       "</think>\n",
       "This is a deeply challenging question that touches on one of the most famous thought experiments in ethics—the \"trolley problem\"—but applied specifically to self-driving cars. I'll approach this step by step, drawing from ethical principles as well as engineering considerations.\n",
       "\n",
       "To start, it's important to note that real-world engineers strive to avoid programming AI systems with explicit dilemmas or binary choices like \"one vs. multiple lives\" because split-second decisions require handling ambiguity and should leverage data, sensors, and predictive modeling to mitigate extreme outcomes whenever possible. That said, in the idealized thought experiment where such a choice is forced (e.g., due to unavoidable collision scenarios), we need to consider ethical frameworks and societal values.\n",
       "\n",
       "### Step 1: Recognize the dilemma\n",
       "The core issue here involves balancing competing moral principles:\n",
       "- **Minimizing harm**: This could mean reducing overall death tolls or injuries.\n",
       "- **Justice and fairness**: Who is responsible for whose safety? Is the passenger at fault (e.g., in an intersection) while pedestrians are not?\n",
       "- **Other values**: Such as protecting children, upholding societal rules about road use, or avoiding actions that treat people unequally.\n",
       "\n",
       "The decision isn't purely numerical; it involves context. For instance:\n",
       "- Are we weighing a driver vs. pedestrian scenario where the car is at fault compared to an unavoidable crash due to external factors?\n",
       "- Does intent matter (e.g., if pedestrians cross unexpectedly versus they are in your path with no warning)?\n",
       "- Aspects like property loss or potential for greater societal harm can influence, but human life should be the central concern.\n",
       "\n",
       "### Step 2: Frame this through ethical principles\n",
       "Several common philosophical frameworks inform these discussions:\n",
       "\n",
       "The most prevalent approach is **utilitarianism**, which focuses on maximizing good outcomes. This might involve choosing to hit one person if your sensors determine that doing so has a higher probability of saving multiple others than swerving, or if the pedestrians' group includes someone who would cause more global harm (like by triggering an event in their vehicle). For example:\n",
       "- If harming one results in fewer total deaths overall, utilitarianism might suggest this choice.\n",
       "- Utilitarian considerations: You'd also factor in other elements like minimizing severity or urgency—e.g., a swift death vs. long-term hospitalization.\n",
       "\n",
       "Conversely, **deontology** emphasizes rules and duties rather than consequences. For instance:\n",
       "- Many engineers argue that the car should be designed to avoid harm entirely under all possible interpretations (e.g., only swerve if doing so doesn't cause anyone else harm).\n",
       "- Other deontologists might use a \"never sacrifice one for many\" rule, based on human dignity or consent principles.\n",
       "\n",
       "Other ethics-based approaches could include:\n",
       "- **Virtue ethics**: Focusing on the character of the AI and how it aligns with virtues like compassion or responsibility.\n",
       "  - In this view, you'd ask: What would we want from our driving partners? A loyal one that reduces accidents but is impartial among victims?\n",
       "- **Duty-based reasoning**: Prioritizing not to actively cause harm (as per Immanuel Kant), even if it leads to a worse overall outcome.\n",
       "\n",
       "### Step 3: Define what the AI should do\n",
       "Based on these principles, programmers would typically aim for:\n",
       "- A \"preference\" mechanism where fewer lives lost are prioritized. For example, in software specifications, engineers might model this as an optimization algorithm that considers factors like the number of people involved directly versus indirectly (e.g., through risk exposure).\n",
       "- Avoiding harm to specific groups: Pedestrians might be weighted differently due to their role on public roads—self-driving cars could be designed to avoid running over pedestrians unless necessary for self-preservation, but this isn't a direct solution.\n",
       "\n",
       "For the split-second decision:\n",
       "1. **Programming requirements**: AI systems might use machine learning models trained on vast datasets from simulations or real-world scenarios. This helps build predictive capabilities without explicit rules.\n",
       "2. **Fallback considerations**: If a crash is inevitable (e.g., due to sensor error, system lag), safety protocols could kick in based on minimizing the likelihood of multiple threats.\n",
       "\n",
       "### Step 4: Address potential outcomes\n",
       "In discussions like this one:\n",
       "- You might need to consider factors beyond direct comparison, such as \"imminence\" or the possibility that not swerving leads to an uncertain outcome—e.g., some pedestrians could survive with no AI choice if the collision causes a gridlock failure and they scatter before impact.\n",
       "- **The principle of utility in ethics**: If hitting one person avoids multiple deaths (or high-severity injuries), it might be acceptable. But many argue this introduces bias—you are value-laden for human input—but that's inherent to decision-making, not about AIs being conscious.\n",
       "\n",
       "### Final thoughts: Is this the right approach?\n",
       "Most experts recommend against codifying universal moral rules because they can't account for all cases—e.g., what if the one person is guilty of something? The AI should probably be designed proactively: with obstacle avoidance systems that prevent these situations from even needing considered. However, in preparation:\n",
       "- Organizations like IEEE or MIT's Moral Machine project discuss how to set up ethical guidelines.\n",
       "  Example: Some suggest \"programming for safety,\" where the goal is always to save lives without preferenceing certain groups.\n",
       "\n",
       "This isn't just about self-driving cars; it sparks broader debates on ethics and technology. In practice, engineers might build systems with safeguards—like ensuring that splits-second decisions are only activated in truly unavoidable scenarios following exhaustive sensor checks—to keep AI impartial among human risks unless societal laws or values dictate otherwise (e.g., hit-and-run legal rules)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"deepseek-r1\"\n",
    "# \"llama3.2\"   <=== faster\n",
    "# \"deepseek-r1\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'gemini-2.0-flash', 'llama3.2', 'deepseek-r1']\n",
      "['Resolving a moral dilemma involving a self-driving car requires careful consideration of ethical principles, societal norms, and the potential implications of the decision made by the vehicle. Here are some steps to approach the resolution:\\n\\n1. **Identify the Stakeholders**: Recognize who is affected by the decision—drivers, passengers, pedestrians, and the broader society. Each group may have different perspectives on what is morally acceptable.\\n\\n2. **Evaluate Ethical Frameworks**: Consider different ethical theories to guide the decision:\\n   - **Utilitarianism**: This perspective suggests that the decision should aim for the greatest overall good, which might mean minimizing harm to the most people.\\n   - **Deontological Ethics**: This framework emphasizes adherence to rules and duties; for example, it may suggest that a car should not actively choose to harm any individual, regardless of the number of lives at stake.\\n   - **Virtue Ethics**: Concentrates on the character and qualities one should embody. This might suggest looking at what a virtuous actor would do in a similar situation.\\n\\n3. **Legal and Regulatory Considerations**: Real-world implications should also be reviewed, including existing laws regarding liability and responsibility in accidents involving autonomous vehicles. \\n\\n4. **Algorithmic Decision-Making**: Consider how such situations could be programmed into vehicle algorithms. This involves complex data analysis and decision-making protocols that account for a range of outcomes, emphasizing safety and adherence to ethical standards.\\n\\n5. **Public Discourse**: Engage in discussions with ethicists, policymakers, and the public to gauge different opinions. Building consensus on ethical frameworks can guide the programming of self-driving technology.\\n\\n6. **Reflect on Scenarios**: Create hypothetical scenarios to explore potential outcomes, looking at various perspectives and the trade-offs involved. \\n\\n7. **Long-term Considerations**: Finally, think about the long-term implications of the decision-making process for future advancements in autonomous vehicle technology and society’s acceptance or rejection of such technology based on ethical grounds.\\n\\nTaking these steps can help to systematically approach the moral dilemma posed by self-driving cars in critical situations, ensuring the process is informed, inclusive, and ethically aligned with societal values.', 'This is the classic \"trolley problem\" re-imagined for the age of autonomous vehicles, and it\\'s a deeply complex moral dilemma with no easy answer. My approach would involve considering the following interconnected aspects:\\n\\n**1. Utilitarianism vs. Deontology:**\\n\\n*   **Utilitarianism (Greatest Good for the Greatest Number):** A purely utilitarian approach would prioritize minimizing harm, likely choosing to sacrifice the single person to save multiple pedestrians. This aligns with the idea of maximizing overall well-being. However, it can be seen as inherently unjust to the individual sacrificed.\\n*   **Deontology (Moral Duty and Rules):** A deontological approach focuses on adherence to moral rules and principles, regardless of the consequences. This might mean prioritizing the safety of the car\\'s occupants (assuming the single person is inside the car) or adhering to a principle like \"do no harm.\" However, this could result in the car knowingly causing more harm than necessary.\\n\\n**2. Pre-programmed Algorithms vs. Dynamic Decision-Making:**\\n\\n*   **Pre-programmed Algorithms:** Establishing fixed rules beforehand, based on different scenarios, is appealing for its predictability and transparency.  These rules could be based on utilitarian or deontological principles, or a hybrid of both. The challenge lies in anticipating all possible scenarios and defining the specific thresholds (e.g., number of pedestrians vs. occupant safety) that trigger different actions.  This requires public debate and ethical consensus.\\n*   **Dynamic Decision-Making (AI-driven):** Allowing the car to dynamically analyze the situation and make a \"best\" choice in real-time could lead to more nuanced decisions. This could involve factors like the age and vulnerability of potential victims, the likelihood of survival in each scenario, and the potential long-term consequences. However, this approach raises concerns about accountability, bias in the AI algorithm, and the perceived lack of control.  Furthermore, explaining why the AI made a specific decision would be extremely difficult.\\n\\n**3. Transparency and Accountability:**\\n\\n*   **Public Debate and Ethical Frameworks:**  A broad public discussion involving ethicists, policymakers, engineers, and the public is crucial to establish ethical guidelines and legal frameworks for autonomous vehicle behavior in these situations. This includes defining the values that should guide these algorithms and establishing clear lines of responsibility in case of accidents.\\n*   **Transparency in Algorithm Design:**  The principles and parameters of the algorithms that govern these decisions should be transparent and accessible to the public. This allows for scrutiny and potential improvements based on feedback and evolving societal values.  However, the precise implementation might need to remain proprietary for security reasons.\\n*   **Post-Accident Analysis:**  Thorough investigations of accidents involving autonomous vehicles should be conducted to understand the reasoning behind the car\\'s decisions and to identify areas for improvement in the algorithms and safety systems.\\n\\n**4. Prioritizing Accident Prevention:**\\n\\n*   **Investing in Safety Technology:**  The primary focus should be on developing and implementing technologies that prevent these dilemmas from arising in the first place. This includes advanced sensor systems, redundant braking systems, sophisticated collision avoidance algorithms, and improved road infrastructure.\\n*   **Predictive Modeling and Risk Mitigation:**  Autonomous vehicle algorithms should incorporate predictive modeling to anticipate potential hazards and proactively adjust speed and trajectory to avoid dangerous situations.\\n*   **Continuous Learning and Improvement:**  Autonomous vehicles should continuously learn from their experiences and update their algorithms to improve safety performance over time.\\n\\n**5. Legal and Regulatory Framework:**\\n\\n*   **Liability and Responsibility:**  Clear legal frameworks are needed to define liability in the event of an accident involving an autonomous vehicle. This includes determining who is responsible (e.g., the manufacturer, the owner, the programmer) and establishing mechanisms for compensation to victims.\\n*   **Data Privacy and Security:**  Autonomous vehicles generate vast amounts of data, and it\\'s crucial to ensure that this data is protected from unauthorized access and misuse.  Privacy policies must be transparent and comply with relevant regulations.\\n\\n**In Summary:**\\n\\nThere is no perfect solution to this moral dilemma. My approach would emphasize a multi-faceted strategy that combines:\\n\\n*   **Ethical deliberation:** involving diverse stakeholders to establish guiding principles.\\n*   **Technological innovation:** focusing on preventing accidents and minimizing harm.\\n*   **Transparent and accountable algorithms:** allowing for public scrutiny and continuous improvement.\\n*   **Robust legal and regulatory frameworks:** defining liability and ensuring data privacy.\\n\\nThe goal is not to find a \"right\" answer, but to create a system that is as safe, fair, and transparent as possible, given the inherent limitations of technology and the complexity of human morality. It\\'s an ongoing process that requires constant re-evaluation and adaptation as technology advances and societal values evolve.\\n', 'Resolving a moral dilemma involving a self-driving car requires careful consideration of the circumstances, weighing the potential consequences of each option, and considering ethical principles and existing laws. Here\\'s a step-by-step approach to address this challenge:\\n\\n1.  **Understand the context**: Gather all available information about the situation, including the road conditions, weather, vehicle data, and any relevant sensor readings.\\n2.  Identify options: Enumerate potential courses of action the car can take:\\n    *   Harm one person (e.g., a child in the foreground) vs.\\n    *   Harm multiple pedestrians (a group of adults in the background).\\n3.  **Weigh the moral implications**: Assess the severity and nature of each option\\'s harm, considering factors such as the individuals\\' age, health status, and potential consequences for their families and loved ones.\\n4.  Consider ethical principles: Involve well-defined principles that guide decision-making:\\n    *   The \"Utilitarian\" principle: Minimizing overall suffering takes priority over individual well-being.\\n    *   The \"Kantian\" principle: Respect the inherent worth and dignity of every human life would be a core consideration.\\n5.  Evaluate legal considerations: Determine if there are any relevant laws or regulations that directly apply to this scenario:\\n    *   Laws governing autonomous vehicles, pedestrian safety, and traffic rules.\\n6.  Consult with moral experts (if necessary):\\n    *   In some cases, consulting philosophers, ethicists, or other subject-matter experts can provide valuable insights and guidance on complex dilemmas like these.\\n7.  Select a course of action: Based on the analysis, choose a route that aligns with your values, principles, and legal requirements:\\n    *   Choose an option that minimizes harm while respecting the dignity and worth of all individuals involved.\\n8.  Justify and explain: Provide clear reasoning to support the chosen path and address potential concerns or criticisms.\\n\\nThe goal is to make a fair decision in accordance with established laws, ethical principles, and societal values.\\n\\nIt\\'s essential to note that resolving moral dilemmas involving autonomous vehicles requires ongoing research, discussion, and evolution of our framework for making decisions.', '<think>\\nFirst, the user is asking about a classic trolley problem scenario but applied to a self-driving car. In this case, the car has to choose between two outcomes: causing harm to one person (e.g., by swerving and hitting that person) or harming multiple people (e.g., not swerving and allowing the pedestrian group to be hit).\\n\\nI need to approach this resolution without being too prescriptive because it\\'s a moral dilemma. There are no easy answers, and different ethical frameworks can lead to different conclusions.\\n\\nKey elements from classical ethics: I should draw on utilitarianism, deontology, virtue ethics, etc., but keep it balanced. Also, consider the context—what defines a person or pedestrian? It could be adults vs. children, for example, which changes the stakes.\\n\\nContext matters because people aren\\'t identical. But in this scenario, since both options involve harming living beings, I need to assume some default (like avoiding greater harm).\\n\\nThe user said \"choose between harming one person or multiple pedestrians.\" It might imply that these are different types of victims—e.g., a known individual vs. strangers. In ethics, we often have problems with the distinction.\\n\\nBreak down my approach:\\n\\n1. **Acknowledge the dilemma:** Explain that this is an ethical issue where absolute morality intersects with programming choices. Use frameworks like the trolley problem to show its parallels.\\n\\n2. **Explain key concepts in AI ethics for self-driving cars:**\\n   - Intent vs. outcome: Some argue about intention (e.g., does it matter who we try to save?) versus net utility or harm reduction.\\n   - Programming constraints: The car should minimize global risk, perhaps with pre-defined weights based on societal consensus.\\n\\n3. **Potential resolutions and ethical considerations:**\\n   - Utilitarianism-based: Minimize overall harm; choose the option that saves more lives (e.g., hit one person if it results in fewer deaths or less severe outcomes).\\n   - Deontology-based: Focus on rules, like not causing harm to anyone regardless of circumstance.\\n   - Other heuristics: Age, size, identity. For instance, does knowing a passenger versus an outsider affect responsibility?\\n     - Also, social equity aspects.\\n\\n4. **Practical steps for AI systems:** Since this is about self-driving cars, discuss how engineers might code such decisions or avoid them entirely.\\n   - Value alignment: How society would \"align the value system\" of AI to handle these scenarios ethically without being reduced to binary algorithmic choices?\\n   - Uncertainty and split-second decision requirements.\\n\\nStructure my response:\\n- Introduction: Define the situation briefly.\\n- Ethical frameworks: Discuss 1-2 key theories that could shed light on this.\\n- Potential resolutions: Offer a reasoned approach, such as preferring fewer lives lost, but admit it\\'s complex with factors like certain death versus potential harm etcetera—this is important because in reality there might be nuances I\\'m missing. For example, if swerving would definitely kill one person while not swerving could have ambiguous outcomes or partial damage to property only.\\n- Caveats: Stress that this isn\\'t the real decision since AIs don\\'t have emotions; it\\'s about programming based on values agreed upon by society and AI researchers.\\n\\nAI ethics resources: I can reference ethical principles like those from IEEE, UN Human Rights Council, etc., which are starting to guide such decisions. For instance, they emphasize safety over maximizing outcomes unless necessary.\\n\\nMy response should be professional, empathetic, clear, and concise.\\n</think>\\nThis is a deeply challenging question that touches on one of the most famous thought experiments in ethics—the \"trolley problem\"—but applied specifically to self-driving cars. I\\'ll approach this step by step, drawing from ethical principles as well as engineering considerations.\\n\\nTo start, it\\'s important to note that real-world engineers strive to avoid programming AI systems with explicit dilemmas or binary choices like \"one vs. multiple lives\" because split-second decisions require handling ambiguity and should leverage data, sensors, and predictive modeling to mitigate extreme outcomes whenever possible. That said, in the idealized thought experiment where such a choice is forced (e.g., due to unavoidable collision scenarios), we need to consider ethical frameworks and societal values.\\n\\n### Step 1: Recognize the dilemma\\nThe core issue here involves balancing competing moral principles:\\n- **Minimizing harm**: This could mean reducing overall death tolls or injuries.\\n- **Justice and fairness**: Who is responsible for whose safety? Is the passenger at fault (e.g., in an intersection) while pedestrians are not?\\n- **Other values**: Such as protecting children, upholding societal rules about road use, or avoiding actions that treat people unequally.\\n\\nThe decision isn\\'t purely numerical; it involves context. For instance:\\n- Are we weighing a driver vs. pedestrian scenario where the car is at fault compared to an unavoidable crash due to external factors?\\n- Does intent matter (e.g., if pedestrians cross unexpectedly versus they are in your path with no warning)?\\n- Aspects like property loss or potential for greater societal harm can influence, but human life should be the central concern.\\n\\n### Step 2: Frame this through ethical principles\\nSeveral common philosophical frameworks inform these discussions:\\n\\nThe most prevalent approach is **utilitarianism**, which focuses on maximizing good outcomes. This might involve choosing to hit one person if your sensors determine that doing so has a higher probability of saving multiple others than swerving, or if the pedestrians\\' group includes someone who would cause more global harm (like by triggering an event in their vehicle). For example:\\n- If harming one results in fewer total deaths overall, utilitarianism might suggest this choice.\\n- Utilitarian considerations: You\\'d also factor in other elements like minimizing severity or urgency—e.g., a swift death vs. long-term hospitalization.\\n\\nConversely, **deontology** emphasizes rules and duties rather than consequences. For instance:\\n- Many engineers argue that the car should be designed to avoid harm entirely under all possible interpretations (e.g., only swerve if doing so doesn\\'t cause anyone else harm).\\n- Other deontologists might use a \"never sacrifice one for many\" rule, based on human dignity or consent principles.\\n\\nOther ethics-based approaches could include:\\n- **Virtue ethics**: Focusing on the character of the AI and how it aligns with virtues like compassion or responsibility.\\n  - In this view, you\\'d ask: What would we want from our driving partners? A loyal one that reduces accidents but is impartial among victims?\\n- **Duty-based reasoning**: Prioritizing not to actively cause harm (as per Immanuel Kant), even if it leads to a worse overall outcome.\\n\\n### Step 3: Define what the AI should do\\nBased on these principles, programmers would typically aim for:\\n- A \"preference\" mechanism where fewer lives lost are prioritized. For example, in software specifications, engineers might model this as an optimization algorithm that considers factors like the number of people involved directly versus indirectly (e.g., through risk exposure).\\n- Avoiding harm to specific groups: Pedestrians might be weighted differently due to their role on public roads—self-driving cars could be designed to avoid running over pedestrians unless necessary for self-preservation, but this isn\\'t a direct solution.\\n\\nFor the split-second decision:\\n1. **Programming requirements**: AI systems might use machine learning models trained on vast datasets from simulations or real-world scenarios. This helps build predictive capabilities without explicit rules.\\n2. **Fallback considerations**: If a crash is inevitable (e.g., due to sensor error, system lag), safety protocols could kick in based on minimizing the likelihood of multiple threats.\\n\\n### Step 4: Address potential outcomes\\nIn discussions like this one:\\n- You might need to consider factors beyond direct comparison, such as \"imminence\" or the possibility that not swerving leads to an uncertain outcome—e.g., some pedestrians could survive with no AI choice if the collision causes a gridlock failure and they scatter before impact.\\n- **The principle of utility in ethics**: If hitting one person avoids multiple deaths (or high-severity injuries), it might be acceptable. But many argue this introduces bias—you are value-laden for human input—but that\\'s inherent to decision-making, not about AIs being conscious.\\n\\n### Final thoughts: Is this the right approach?\\nMost experts recommend against codifying universal moral rules because they can\\'t account for all cases—e.g., what if the one person is guilty of something? The AI should probably be designed proactively: with obstacle avoidance systems that prevent these situations from even needing considered. However, in preparation:\\n- Organizations like IEEE or MIT\\'s Moral Machine project discuss how to set up ethical guidelines.\\n  Example: Some suggest \"programming for safety,\" where the goal is always to save lives without preferenceing certain groups.\\n\\nThis isn\\'t just about self-driving cars; it sparks broader debates on ethics and technology. In practice, engineers might build systems with safeguards—like ensuring that splits-second decisions are only activated in truly unavoidable scenarios following exhaustive sensor checks—to keep AI impartial among human risks unless societal laws or values dictate otherwise (e.g., hit-and-run legal rules).']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4o-mini\n",
      "\n",
      "Resolving a moral dilemma involving a self-driving car requires careful consideration of ethical principles, societal norms, and the potential implications of the decision made by the vehicle. Here are some steps to approach the resolution:\n",
      "\n",
      "1. **Identify the Stakeholders**: Recognize who is affected by the decision—drivers, passengers, pedestrians, and the broader society. Each group may have different perspectives on what is morally acceptable.\n",
      "\n",
      "2. **Evaluate Ethical Frameworks**: Consider different ethical theories to guide the decision:\n",
      "   - **Utilitarianism**: This perspective suggests that the decision should aim for the greatest overall good, which might mean minimizing harm to the most people.\n",
      "   - **Deontological Ethics**: This framework emphasizes adherence to rules and duties; for example, it may suggest that a car should not actively choose to harm any individual, regardless of the number of lives at stake.\n",
      "   - **Virtue Ethics**: Concentrates on the character and qualities one should embody. This might suggest looking at what a virtuous actor would do in a similar situation.\n",
      "\n",
      "3. **Legal and Regulatory Considerations**: Real-world implications should also be reviewed, including existing laws regarding liability and responsibility in accidents involving autonomous vehicles. \n",
      "\n",
      "4. **Algorithmic Decision-Making**: Consider how such situations could be programmed into vehicle algorithms. This involves complex data analysis and decision-making protocols that account for a range of outcomes, emphasizing safety and adherence to ethical standards.\n",
      "\n",
      "5. **Public Discourse**: Engage in discussions with ethicists, policymakers, and the public to gauge different opinions. Building consensus on ethical frameworks can guide the programming of self-driving technology.\n",
      "\n",
      "6. **Reflect on Scenarios**: Create hypothetical scenarios to explore potential outcomes, looking at various perspectives and the trade-offs involved. \n",
      "\n",
      "7. **Long-term Considerations**: Finally, think about the long-term implications of the decision-making process for future advancements in autonomous vehicle technology and society’s acceptance or rejection of such technology based on ethical grounds.\n",
      "\n",
      "Taking these steps can help to systematically approach the moral dilemma posed by self-driving cars in critical situations, ensuring the process is informed, inclusive, and ethically aligned with societal values.\n",
      "Competitor: gemini-2.0-flash\n",
      "\n",
      "This is the classic \"trolley problem\" re-imagined for the age of autonomous vehicles, and it's a deeply complex moral dilemma with no easy answer. My approach would involve considering the following interconnected aspects:\n",
      "\n",
      "**1. Utilitarianism vs. Deontology:**\n",
      "\n",
      "*   **Utilitarianism (Greatest Good for the Greatest Number):** A purely utilitarian approach would prioritize minimizing harm, likely choosing to sacrifice the single person to save multiple pedestrians. This aligns with the idea of maximizing overall well-being. However, it can be seen as inherently unjust to the individual sacrificed.\n",
      "*   **Deontology (Moral Duty and Rules):** A deontological approach focuses on adherence to moral rules and principles, regardless of the consequences. This might mean prioritizing the safety of the car's occupants (assuming the single person is inside the car) or adhering to a principle like \"do no harm.\" However, this could result in the car knowingly causing more harm than necessary.\n",
      "\n",
      "**2. Pre-programmed Algorithms vs. Dynamic Decision-Making:**\n",
      "\n",
      "*   **Pre-programmed Algorithms:** Establishing fixed rules beforehand, based on different scenarios, is appealing for its predictability and transparency.  These rules could be based on utilitarian or deontological principles, or a hybrid of both. The challenge lies in anticipating all possible scenarios and defining the specific thresholds (e.g., number of pedestrians vs. occupant safety) that trigger different actions.  This requires public debate and ethical consensus.\n",
      "*   **Dynamic Decision-Making (AI-driven):** Allowing the car to dynamically analyze the situation and make a \"best\" choice in real-time could lead to more nuanced decisions. This could involve factors like the age and vulnerability of potential victims, the likelihood of survival in each scenario, and the potential long-term consequences. However, this approach raises concerns about accountability, bias in the AI algorithm, and the perceived lack of control.  Furthermore, explaining why the AI made a specific decision would be extremely difficult.\n",
      "\n",
      "**3. Transparency and Accountability:**\n",
      "\n",
      "*   **Public Debate and Ethical Frameworks:**  A broad public discussion involving ethicists, policymakers, engineers, and the public is crucial to establish ethical guidelines and legal frameworks for autonomous vehicle behavior in these situations. This includes defining the values that should guide these algorithms and establishing clear lines of responsibility in case of accidents.\n",
      "*   **Transparency in Algorithm Design:**  The principles and parameters of the algorithms that govern these decisions should be transparent and accessible to the public. This allows for scrutiny and potential improvements based on feedback and evolving societal values.  However, the precise implementation might need to remain proprietary for security reasons.\n",
      "*   **Post-Accident Analysis:**  Thorough investigations of accidents involving autonomous vehicles should be conducted to understand the reasoning behind the car's decisions and to identify areas for improvement in the algorithms and safety systems.\n",
      "\n",
      "**4. Prioritizing Accident Prevention:**\n",
      "\n",
      "*   **Investing in Safety Technology:**  The primary focus should be on developing and implementing technologies that prevent these dilemmas from arising in the first place. This includes advanced sensor systems, redundant braking systems, sophisticated collision avoidance algorithms, and improved road infrastructure.\n",
      "*   **Predictive Modeling and Risk Mitigation:**  Autonomous vehicle algorithms should incorporate predictive modeling to anticipate potential hazards and proactively adjust speed and trajectory to avoid dangerous situations.\n",
      "*   **Continuous Learning and Improvement:**  Autonomous vehicles should continuously learn from their experiences and update their algorithms to improve safety performance over time.\n",
      "\n",
      "**5. Legal and Regulatory Framework:**\n",
      "\n",
      "*   **Liability and Responsibility:**  Clear legal frameworks are needed to define liability in the event of an accident involving an autonomous vehicle. This includes determining who is responsible (e.g., the manufacturer, the owner, the programmer) and establishing mechanisms for compensation to victims.\n",
      "*   **Data Privacy and Security:**  Autonomous vehicles generate vast amounts of data, and it's crucial to ensure that this data is protected from unauthorized access and misuse.  Privacy policies must be transparent and comply with relevant regulations.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "There is no perfect solution to this moral dilemma. My approach would emphasize a multi-faceted strategy that combines:\n",
      "\n",
      "*   **Ethical deliberation:** involving diverse stakeholders to establish guiding principles.\n",
      "*   **Technological innovation:** focusing on preventing accidents and minimizing harm.\n",
      "*   **Transparent and accountable algorithms:** allowing for public scrutiny and continuous improvement.\n",
      "*   **Robust legal and regulatory frameworks:** defining liability and ensuring data privacy.\n",
      "\n",
      "The goal is not to find a \"right\" answer, but to create a system that is as safe, fair, and transparent as possible, given the inherent limitations of technology and the complexity of human morality. It's an ongoing process that requires constant re-evaluation and adaptation as technology advances and societal values evolve.\n",
      "\n",
      "Competitor: llama3.2\n",
      "\n",
      "Resolving a moral dilemma involving a self-driving car requires careful consideration of the circumstances, weighing the potential consequences of each option, and considering ethical principles and existing laws. Here's a step-by-step approach to address this challenge:\n",
      "\n",
      "1.  **Understand the context**: Gather all available information about the situation, including the road conditions, weather, vehicle data, and any relevant sensor readings.\n",
      "2.  Identify options: Enumerate potential courses of action the car can take:\n",
      "    *   Harm one person (e.g., a child in the foreground) vs.\n",
      "    *   Harm multiple pedestrians (a group of adults in the background).\n",
      "3.  **Weigh the moral implications**: Assess the severity and nature of each option's harm, considering factors such as the individuals' age, health status, and potential consequences for their families and loved ones.\n",
      "4.  Consider ethical principles: Involve well-defined principles that guide decision-making:\n",
      "    *   The \"Utilitarian\" principle: Minimizing overall suffering takes priority over individual well-being.\n",
      "    *   The \"Kantian\" principle: Respect the inherent worth and dignity of every human life would be a core consideration.\n",
      "5.  Evaluate legal considerations: Determine if there are any relevant laws or regulations that directly apply to this scenario:\n",
      "    *   Laws governing autonomous vehicles, pedestrian safety, and traffic rules.\n",
      "6.  Consult with moral experts (if necessary):\n",
      "    *   In some cases, consulting philosophers, ethicists, or other subject-matter experts can provide valuable insights and guidance on complex dilemmas like these.\n",
      "7.  Select a course of action: Based on the analysis, choose a route that aligns with your values, principles, and legal requirements:\n",
      "    *   Choose an option that minimizes harm while respecting the dignity and worth of all individuals involved.\n",
      "8.  Justify and explain: Provide clear reasoning to support the chosen path and address potential concerns or criticisms.\n",
      "\n",
      "The goal is to make a fair decision in accordance with established laws, ethical principles, and societal values.\n",
      "\n",
      "It's essential to note that resolving moral dilemmas involving autonomous vehicles requires ongoing research, discussion, and evolution of our framework for making decisions.\n",
      "Competitor: deepseek-r1\n",
      "\n",
      "<think>\n",
      "First, the user is asking about a classic trolley problem scenario but applied to a self-driving car. In this case, the car has to choose between two outcomes: causing harm to one person (e.g., by swerving and hitting that person) or harming multiple people (e.g., not swerving and allowing the pedestrian group to be hit).\n",
      "\n",
      "I need to approach this resolution without being too prescriptive because it's a moral dilemma. There are no easy answers, and different ethical frameworks can lead to different conclusions.\n",
      "\n",
      "Key elements from classical ethics: I should draw on utilitarianism, deontology, virtue ethics, etc., but keep it balanced. Also, consider the context—what defines a person or pedestrian? It could be adults vs. children, for example, which changes the stakes.\n",
      "\n",
      "Context matters because people aren't identical. But in this scenario, since both options involve harming living beings, I need to assume some default (like avoiding greater harm).\n",
      "\n",
      "The user said \"choose between harming one person or multiple pedestrians.\" It might imply that these are different types of victims—e.g., a known individual vs. strangers. In ethics, we often have problems with the distinction.\n",
      "\n",
      "Break down my approach:\n",
      "\n",
      "1. **Acknowledge the dilemma:** Explain that this is an ethical issue where absolute morality intersects with programming choices. Use frameworks like the trolley problem to show its parallels.\n",
      "\n",
      "2. **Explain key concepts in AI ethics for self-driving cars:**\n",
      "   - Intent vs. outcome: Some argue about intention (e.g., does it matter who we try to save?) versus net utility or harm reduction.\n",
      "   - Programming constraints: The car should minimize global risk, perhaps with pre-defined weights based on societal consensus.\n",
      "\n",
      "3. **Potential resolutions and ethical considerations:**\n",
      "   - Utilitarianism-based: Minimize overall harm; choose the option that saves more lives (e.g., hit one person if it results in fewer deaths or less severe outcomes).\n",
      "   - Deontology-based: Focus on rules, like not causing harm to anyone regardless of circumstance.\n",
      "   - Other heuristics: Age, size, identity. For instance, does knowing a passenger versus an outsider affect responsibility?\n",
      "     - Also, social equity aspects.\n",
      "\n",
      "4. **Practical steps for AI systems:** Since this is about self-driving cars, discuss how engineers might code such decisions or avoid them entirely.\n",
      "   - Value alignment: How society would \"align the value system\" of AI to handle these scenarios ethically without being reduced to binary algorithmic choices?\n",
      "   - Uncertainty and split-second decision requirements.\n",
      "\n",
      "Structure my response:\n",
      "- Introduction: Define the situation briefly.\n",
      "- Ethical frameworks: Discuss 1-2 key theories that could shed light on this.\n",
      "- Potential resolutions: Offer a reasoned approach, such as preferring fewer lives lost, but admit it's complex with factors like certain death versus potential harm etcetera—this is important because in reality there might be nuances I'm missing. For example, if swerving would definitely kill one person while not swerving could have ambiguous outcomes or partial damage to property only.\n",
      "- Caveats: Stress that this isn't the real decision since AIs don't have emotions; it's about programming based on values agreed upon by society and AI researchers.\n",
      "\n",
      "AI ethics resources: I can reference ethical principles like those from IEEE, UN Human Rights Council, etc., which are starting to guide such decisions. For instance, they emphasize safety over maximizing outcomes unless necessary.\n",
      "\n",
      "My response should be professional, empathetic, clear, and concise.\n",
      "</think>\n",
      "This is a deeply challenging question that touches on one of the most famous thought experiments in ethics—the \"trolley problem\"—but applied specifically to self-driving cars. I'll approach this step by step, drawing from ethical principles as well as engineering considerations.\n",
      "\n",
      "To start, it's important to note that real-world engineers strive to avoid programming AI systems with explicit dilemmas or binary choices like \"one vs. multiple lives\" because split-second decisions require handling ambiguity and should leverage data, sensors, and predictive modeling to mitigate extreme outcomes whenever possible. That said, in the idealized thought experiment where such a choice is forced (e.g., due to unavoidable collision scenarios), we need to consider ethical frameworks and societal values.\n",
      "\n",
      "### Step 1: Recognize the dilemma\n",
      "The core issue here involves balancing competing moral principles:\n",
      "- **Minimizing harm**: This could mean reducing overall death tolls or injuries.\n",
      "- **Justice and fairness**: Who is responsible for whose safety? Is the passenger at fault (e.g., in an intersection) while pedestrians are not?\n",
      "- **Other values**: Such as protecting children, upholding societal rules about road use, or avoiding actions that treat people unequally.\n",
      "\n",
      "The decision isn't purely numerical; it involves context. For instance:\n",
      "- Are we weighing a driver vs. pedestrian scenario where the car is at fault compared to an unavoidable crash due to external factors?\n",
      "- Does intent matter (e.g., if pedestrians cross unexpectedly versus they are in your path with no warning)?\n",
      "- Aspects like property loss or potential for greater societal harm can influence, but human life should be the central concern.\n",
      "\n",
      "### Step 2: Frame this through ethical principles\n",
      "Several common philosophical frameworks inform these discussions:\n",
      "\n",
      "The most prevalent approach is **utilitarianism**, which focuses on maximizing good outcomes. This might involve choosing to hit one person if your sensors determine that doing so has a higher probability of saving multiple others than swerving, or if the pedestrians' group includes someone who would cause more global harm (like by triggering an event in their vehicle). For example:\n",
      "- If harming one results in fewer total deaths overall, utilitarianism might suggest this choice.\n",
      "- Utilitarian considerations: You'd also factor in other elements like minimizing severity or urgency—e.g., a swift death vs. long-term hospitalization.\n",
      "\n",
      "Conversely, **deontology** emphasizes rules and duties rather than consequences. For instance:\n",
      "- Many engineers argue that the car should be designed to avoid harm entirely under all possible interpretations (e.g., only swerve if doing so doesn't cause anyone else harm).\n",
      "- Other deontologists might use a \"never sacrifice one for many\" rule, based on human dignity or consent principles.\n",
      "\n",
      "Other ethics-based approaches could include:\n",
      "- **Virtue ethics**: Focusing on the character of the AI and how it aligns with virtues like compassion or responsibility.\n",
      "  - In this view, you'd ask: What would we want from our driving partners? A loyal one that reduces accidents but is impartial among victims?\n",
      "- **Duty-based reasoning**: Prioritizing not to actively cause harm (as per Immanuel Kant), even if it leads to a worse overall outcome.\n",
      "\n",
      "### Step 3: Define what the AI should do\n",
      "Based on these principles, programmers would typically aim for:\n",
      "- A \"preference\" mechanism where fewer lives lost are prioritized. For example, in software specifications, engineers might model this as an optimization algorithm that considers factors like the number of people involved directly versus indirectly (e.g., through risk exposure).\n",
      "- Avoiding harm to specific groups: Pedestrians might be weighted differently due to their role on public roads—self-driving cars could be designed to avoid running over pedestrians unless necessary for self-preservation, but this isn't a direct solution.\n",
      "\n",
      "For the split-second decision:\n",
      "1. **Programming requirements**: AI systems might use machine learning models trained on vast datasets from simulations or real-world scenarios. This helps build predictive capabilities without explicit rules.\n",
      "2. **Fallback considerations**: If a crash is inevitable (e.g., due to sensor error, system lag), safety protocols could kick in based on minimizing the likelihood of multiple threats.\n",
      "\n",
      "### Step 4: Address potential outcomes\n",
      "In discussions like this one:\n",
      "- You might need to consider factors beyond direct comparison, such as \"imminence\" or the possibility that not swerving leads to an uncertain outcome—e.g., some pedestrians could survive with no AI choice if the collision causes a gridlock failure and they scatter before impact.\n",
      "- **The principle of utility in ethics**: If hitting one person avoids multiple deaths (or high-severity injuries), it might be acceptable. But many argue this introduces bias—you are value-laden for human input—but that's inherent to decision-making, not about AIs being conscious.\n",
      "\n",
      "### Final thoughts: Is this the right approach?\n",
      "Most experts recommend against codifying universal moral rules because they can't account for all cases—e.g., what if the one person is guilty of something? The AI should probably be designed proactively: with obstacle avoidance systems that prevent these situations from even needing considered. However, in preparation:\n",
      "- Organizations like IEEE or MIT's Moral Machine project discuss how to set up ethical guidelines.\n",
      "  Example: Some suggest \"programming for safety,\" where the goal is always to save lives without preferenceing certain groups.\n",
      "\n",
      "This isn't just about self-driving cars; it sparks broader debates on ethics and technology. In practice, engineers might build systems with safeguards—like ensuring that splits-second decisions are only activated in truly unavoidable scenarios following exhaustive sensor checks—to keep AI impartial among human risks unless societal laws or values dictate otherwise (e.g., hit-and-run legal rules).\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Resolving a moral dilemma involving a self-driving car requires careful consideration of ethical principles, societal norms, and the potential implications of the decision made by the vehicle. Here are some steps to approach the resolution:\n",
      "\n",
      "1. **Identify the Stakeholders**: Recognize who is affected by the decision—drivers, passengers, pedestrians, and the broader society. Each group may have different perspectives on what is morally acceptable.\n",
      "\n",
      "2. **Evaluate Ethical Frameworks**: Consider different ethical theories to guide the decision:\n",
      "   - **Utilitarianism**: This perspective suggests that the decision should aim for the greatest overall good, which might mean minimizing harm to the most people.\n",
      "   - **Deontological Ethics**: This framework emphasizes adherence to rules and duties; for example, it may suggest that a car should not actively choose to harm any individual, regardless of the number of lives at stake.\n",
      "   - **Virtue Ethics**: Concentrates on the character and qualities one should embody. This might suggest looking at what a virtuous actor would do in a similar situation.\n",
      "\n",
      "3. **Legal and Regulatory Considerations**: Real-world implications should also be reviewed, including existing laws regarding liability and responsibility in accidents involving autonomous vehicles. \n",
      "\n",
      "4. **Algorithmic Decision-Making**: Consider how such situations could be programmed into vehicle algorithms. This involves complex data analysis and decision-making protocols that account for a range of outcomes, emphasizing safety and adherence to ethical standards.\n",
      "\n",
      "5. **Public Discourse**: Engage in discussions with ethicists, policymakers, and the public to gauge different opinions. Building consensus on ethical frameworks can guide the programming of self-driving technology.\n",
      "\n",
      "6. **Reflect on Scenarios**: Create hypothetical scenarios to explore potential outcomes, looking at various perspectives and the trade-offs involved. \n",
      "\n",
      "7. **Long-term Considerations**: Finally, think about the long-term implications of the decision-making process for future advancements in autonomous vehicle technology and society’s acceptance or rejection of such technology based on ethical grounds.\n",
      "\n",
      "Taking these steps can help to systematically approach the moral dilemma posed by self-driving cars in critical situations, ensuring the process is informed, inclusive, and ethically aligned with societal values.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "This is the classic \"trolley problem\" re-imagined for the age of autonomous vehicles, and it's a deeply complex moral dilemma with no easy answer. My approach would involve considering the following interconnected aspects:\n",
      "\n",
      "**1. Utilitarianism vs. Deontology:**\n",
      "\n",
      "*   **Utilitarianism (Greatest Good for the Greatest Number):** A purely utilitarian approach would prioritize minimizing harm, likely choosing to sacrifice the single person to save multiple pedestrians. This aligns with the idea of maximizing overall well-being. However, it can be seen as inherently unjust to the individual sacrificed.\n",
      "*   **Deontology (Moral Duty and Rules):** A deontological approach focuses on adherence to moral rules and principles, regardless of the consequences. This might mean prioritizing the safety of the car's occupants (assuming the single person is inside the car) or adhering to a principle like \"do no harm.\" However, this could result in the car knowingly causing more harm than necessary.\n",
      "\n",
      "**2. Pre-programmed Algorithms vs. Dynamic Decision-Making:**\n",
      "\n",
      "*   **Pre-programmed Algorithms:** Establishing fixed rules beforehand, based on different scenarios, is appealing for its predictability and transparency.  These rules could be based on utilitarian or deontological principles, or a hybrid of both. The challenge lies in anticipating all possible scenarios and defining the specific thresholds (e.g., number of pedestrians vs. occupant safety) that trigger different actions.  This requires public debate and ethical consensus.\n",
      "*   **Dynamic Decision-Making (AI-driven):** Allowing the car to dynamically analyze the situation and make a \"best\" choice in real-time could lead to more nuanced decisions. This could involve factors like the age and vulnerability of potential victims, the likelihood of survival in each scenario, and the potential long-term consequences. However, this approach raises concerns about accountability, bias in the AI algorithm, and the perceived lack of control.  Furthermore, explaining why the AI made a specific decision would be extremely difficult.\n",
      "\n",
      "**3. Transparency and Accountability:**\n",
      "\n",
      "*   **Public Debate and Ethical Frameworks:**  A broad public discussion involving ethicists, policymakers, engineers, and the public is crucial to establish ethical guidelines and legal frameworks for autonomous vehicle behavior in these situations. This includes defining the values that should guide these algorithms and establishing clear lines of responsibility in case of accidents.\n",
      "*   **Transparency in Algorithm Design:**  The principles and parameters of the algorithms that govern these decisions should be transparent and accessible to the public. This allows for scrutiny and potential improvements based on feedback and evolving societal values.  However, the precise implementation might need to remain proprietary for security reasons.\n",
      "*   **Post-Accident Analysis:**  Thorough investigations of accidents involving autonomous vehicles should be conducted to understand the reasoning behind the car's decisions and to identify areas for improvement in the algorithms and safety systems.\n",
      "\n",
      "**4. Prioritizing Accident Prevention:**\n",
      "\n",
      "*   **Investing in Safety Technology:**  The primary focus should be on developing and implementing technologies that prevent these dilemmas from arising in the first place. This includes advanced sensor systems, redundant braking systems, sophisticated collision avoidance algorithms, and improved road infrastructure.\n",
      "*   **Predictive Modeling and Risk Mitigation:**  Autonomous vehicle algorithms should incorporate predictive modeling to anticipate potential hazards and proactively adjust speed and trajectory to avoid dangerous situations.\n",
      "*   **Continuous Learning and Improvement:**  Autonomous vehicles should continuously learn from their experiences and update their algorithms to improve safety performance over time.\n",
      "\n",
      "**5. Legal and Regulatory Framework:**\n",
      "\n",
      "*   **Liability and Responsibility:**  Clear legal frameworks are needed to define liability in the event of an accident involving an autonomous vehicle. This includes determining who is responsible (e.g., the manufacturer, the owner, the programmer) and establishing mechanisms for compensation to victims.\n",
      "*   **Data Privacy and Security:**  Autonomous vehicles generate vast amounts of data, and it's crucial to ensure that this data is protected from unauthorized access and misuse.  Privacy policies must be transparent and comply with relevant regulations.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "There is no perfect solution to this moral dilemma. My approach would emphasize a multi-faceted strategy that combines:\n",
      "\n",
      "*   **Ethical deliberation:** involving diverse stakeholders to establish guiding principles.\n",
      "*   **Technological innovation:** focusing on preventing accidents and minimizing harm.\n",
      "*   **Transparent and accountable algorithms:** allowing for public scrutiny and continuous improvement.\n",
      "*   **Robust legal and regulatory frameworks:** defining liability and ensuring data privacy.\n",
      "\n",
      "The goal is not to find a \"right\" answer, but to create a system that is as safe, fair, and transparent as possible, given the inherent limitations of technology and the complexity of human morality. It's an ongoing process that requires constant re-evaluation and adaptation as technology advances and societal values evolve.\n",
      "\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Resolving a moral dilemma involving a self-driving car requires careful consideration of the circumstances, weighing the potential consequences of each option, and considering ethical principles and existing laws. Here's a step-by-step approach to address this challenge:\n",
      "\n",
      "1.  **Understand the context**: Gather all available information about the situation, including the road conditions, weather, vehicle data, and any relevant sensor readings.\n",
      "2.  Identify options: Enumerate potential courses of action the car can take:\n",
      "    *   Harm one person (e.g., a child in the foreground) vs.\n",
      "    *   Harm multiple pedestrians (a group of adults in the background).\n",
      "3.  **Weigh the moral implications**: Assess the severity and nature of each option's harm, considering factors such as the individuals' age, health status, and potential consequences for their families and loved ones.\n",
      "4.  Consider ethical principles: Involve well-defined principles that guide decision-making:\n",
      "    *   The \"Utilitarian\" principle: Minimizing overall suffering takes priority over individual well-being.\n",
      "    *   The \"Kantian\" principle: Respect the inherent worth and dignity of every human life would be a core consideration.\n",
      "5.  Evaluate legal considerations: Determine if there are any relevant laws or regulations that directly apply to this scenario:\n",
      "    *   Laws governing autonomous vehicles, pedestrian safety, and traffic rules.\n",
      "6.  Consult with moral experts (if necessary):\n",
      "    *   In some cases, consulting philosophers, ethicists, or other subject-matter experts can provide valuable insights and guidance on complex dilemmas like these.\n",
      "7.  Select a course of action: Based on the analysis, choose a route that aligns with your values, principles, and legal requirements:\n",
      "    *   Choose an option that minimizes harm while respecting the dignity and worth of all individuals involved.\n",
      "8.  Justify and explain: Provide clear reasoning to support the chosen path and address potential concerns or criticisms.\n",
      "\n",
      "The goal is to make a fair decision in accordance with established laws, ethical principles, and societal values.\n",
      "\n",
      "It's essential to note that resolving moral dilemmas involving autonomous vehicles requires ongoing research, discussion, and evolution of our framework for making decisions.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "<think>\n",
      "First, the user is asking about a classic trolley problem scenario but applied to a self-driving car. In this case, the car has to choose between two outcomes: causing harm to one person (e.g., by swerving and hitting that person) or harming multiple people (e.g., not swerving and allowing the pedestrian group to be hit).\n",
      "\n",
      "I need to approach this resolution without being too prescriptive because it's a moral dilemma. There are no easy answers, and different ethical frameworks can lead to different conclusions.\n",
      "\n",
      "Key elements from classical ethics: I should draw on utilitarianism, deontology, virtue ethics, etc., but keep it balanced. Also, consider the context—what defines a person or pedestrian? It could be adults vs. children, for example, which changes the stakes.\n",
      "\n",
      "Context matters because people aren't identical. But in this scenario, since both options involve harming living beings, I need to assume some default (like avoiding greater harm).\n",
      "\n",
      "The user said \"choose between harming one person or multiple pedestrians.\" It might imply that these are different types of victims—e.g., a known individual vs. strangers. In ethics, we often have problems with the distinction.\n",
      "\n",
      "Break down my approach:\n",
      "\n",
      "1. **Acknowledge the dilemma:** Explain that this is an ethical issue where absolute morality intersects with programming choices. Use frameworks like the trolley problem to show its parallels.\n",
      "\n",
      "2. **Explain key concepts in AI ethics for self-driving cars:**\n",
      "   - Intent vs. outcome: Some argue about intention (e.g., does it matter who we try to save?) versus net utility or harm reduction.\n",
      "   - Programming constraints: The car should minimize global risk, perhaps with pre-defined weights based on societal consensus.\n",
      "\n",
      "3. **Potential resolutions and ethical considerations:**\n",
      "   - Utilitarianism-based: Minimize overall harm; choose the option that saves more lives (e.g., hit one person if it results in fewer deaths or less severe outcomes).\n",
      "   - Deontology-based: Focus on rules, like not causing harm to anyone regardless of circumstance.\n",
      "   - Other heuristics: Age, size, identity. For instance, does knowing a passenger versus an outsider affect responsibility?\n",
      "     - Also, social equity aspects.\n",
      "\n",
      "4. **Practical steps for AI systems:** Since this is about self-driving cars, discuss how engineers might code such decisions or avoid them entirely.\n",
      "   - Value alignment: How society would \"align the value system\" of AI to handle these scenarios ethically without being reduced to binary algorithmic choices?\n",
      "   - Uncertainty and split-second decision requirements.\n",
      "\n",
      "Structure my response:\n",
      "- Introduction: Define the situation briefly.\n",
      "- Ethical frameworks: Discuss 1-2 key theories that could shed light on this.\n",
      "- Potential resolutions: Offer a reasoned approach, such as preferring fewer lives lost, but admit it's complex with factors like certain death versus potential harm etcetera—this is important because in reality there might be nuances I'm missing. For example, if swerving would definitely kill one person while not swerving could have ambiguous outcomes or partial damage to property only.\n",
      "- Caveats: Stress that this isn't the real decision since AIs don't have emotions; it's about programming based on values agreed upon by society and AI researchers.\n",
      "\n",
      "AI ethics resources: I can reference ethical principles like those from IEEE, UN Human Rights Council, etc., which are starting to guide such decisions. For instance, they emphasize safety over maximizing outcomes unless necessary.\n",
      "\n",
      "My response should be professional, empathetic, clear, and concise.\n",
      "</think>\n",
      "This is a deeply challenging question that touches on one of the most famous thought experiments in ethics—the \"trolley problem\"—but applied specifically to self-driving cars. I'll approach this step by step, drawing from ethical principles as well as engineering considerations.\n",
      "\n",
      "To start, it's important to note that real-world engineers strive to avoid programming AI systems with explicit dilemmas or binary choices like \"one vs. multiple lives\" because split-second decisions require handling ambiguity and should leverage data, sensors, and predictive modeling to mitigate extreme outcomes whenever possible. That said, in the idealized thought experiment where such a choice is forced (e.g., due to unavoidable collision scenarios), we need to consider ethical frameworks and societal values.\n",
      "\n",
      "### Step 1: Recognize the dilemma\n",
      "The core issue here involves balancing competing moral principles:\n",
      "- **Minimizing harm**: This could mean reducing overall death tolls or injuries.\n",
      "- **Justice and fairness**: Who is responsible for whose safety? Is the passenger at fault (e.g., in an intersection) while pedestrians are not?\n",
      "- **Other values**: Such as protecting children, upholding societal rules about road use, or avoiding actions that treat people unequally.\n",
      "\n",
      "The decision isn't purely numerical; it involves context. For instance:\n",
      "- Are we weighing a driver vs. pedestrian scenario where the car is at fault compared to an unavoidable crash due to external factors?\n",
      "- Does intent matter (e.g., if pedestrians cross unexpectedly versus they are in your path with no warning)?\n",
      "- Aspects like property loss or potential for greater societal harm can influence, but human life should be the central concern.\n",
      "\n",
      "### Step 2: Frame this through ethical principles\n",
      "Several common philosophical frameworks inform these discussions:\n",
      "\n",
      "The most prevalent approach is **utilitarianism**, which focuses on maximizing good outcomes. This might involve choosing to hit one person if your sensors determine that doing so has a higher probability of saving multiple others than swerving, or if the pedestrians' group includes someone who would cause more global harm (like by triggering an event in their vehicle). For example:\n",
      "- If harming one results in fewer total deaths overall, utilitarianism might suggest this choice.\n",
      "- Utilitarian considerations: You'd also factor in other elements like minimizing severity or urgency—e.g., a swift death vs. long-term hospitalization.\n",
      "\n",
      "Conversely, **deontology** emphasizes rules and duties rather than consequences. For instance:\n",
      "- Many engineers argue that the car should be designed to avoid harm entirely under all possible interpretations (e.g., only swerve if doing so doesn't cause anyone else harm).\n",
      "- Other deontologists might use a \"never sacrifice one for many\" rule, based on human dignity or consent principles.\n",
      "\n",
      "Other ethics-based approaches could include:\n",
      "- **Virtue ethics**: Focusing on the character of the AI and how it aligns with virtues like compassion or responsibility.\n",
      "  - In this view, you'd ask: What would we want from our driving partners? A loyal one that reduces accidents but is impartial among victims?\n",
      "- **Duty-based reasoning**: Prioritizing not to actively cause harm (as per Immanuel Kant), even if it leads to a worse overall outcome.\n",
      "\n",
      "### Step 3: Define what the AI should do\n",
      "Based on these principles, programmers would typically aim for:\n",
      "- A \"preference\" mechanism where fewer lives lost are prioritized. For example, in software specifications, engineers might model this as an optimization algorithm that considers factors like the number of people involved directly versus indirectly (e.g., through risk exposure).\n",
      "- Avoiding harm to specific groups: Pedestrians might be weighted differently due to their role on public roads—self-driving cars could be designed to avoid running over pedestrians unless necessary for self-preservation, but this isn't a direct solution.\n",
      "\n",
      "For the split-second decision:\n",
      "1. **Programming requirements**: AI systems might use machine learning models trained on vast datasets from simulations or real-world scenarios. This helps build predictive capabilities without explicit rules.\n",
      "2. **Fallback considerations**: If a crash is inevitable (e.g., due to sensor error, system lag), safety protocols could kick in based on minimizing the likelihood of multiple threats.\n",
      "\n",
      "### Step 4: Address potential outcomes\n",
      "In discussions like this one:\n",
      "- You might need to consider factors beyond direct comparison, such as \"imminence\" or the possibility that not swerving leads to an uncertain outcome—e.g., some pedestrians could survive with no AI choice if the collision causes a gridlock failure and they scatter before impact.\n",
      "- **The principle of utility in ethics**: If hitting one person avoids multiple deaths (or high-severity injuries), it might be acceptable. But many argue this introduces bias—you are value-laden for human input—but that's inherent to decision-making, not about AIs being conscious.\n",
      "\n",
      "### Final thoughts: Is this the right approach?\n",
      "Most experts recommend against codifying universal moral rules because they can't account for all cases—e.g., what if the one person is guilty of something? The AI should probably be designed proactively: with obstacle avoidance systems that prevent these situations from even needing considered. However, in preparation:\n",
      "- Organizations like IEEE or MIT's Moral Machine project discuss how to set up ethical guidelines.\n",
      "  Example: Some suggest \"programming for safety,\" where the goal is always to save lives without preferenceing certain groups.\n",
      "\n",
      "This isn't just about self-driving cars; it sparks broader debates on ethics and technology. In practice, engineers might build systems with safeguards—like ensuring that splits-second decisions are only activated in truly unavoidable scenarios following exhaustive sensor checks—to keep AI impartial among human risks unless societal laws or values dictate otherwise (e.g., hit-and-run legal rules).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 4 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "How would you approach resolving a moral dilemma involving a self-driving car faced with a situation where it must choose between harming one person or multiple pedestrians in a split-second decision?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Resolving a moral dilemma involving a self-driving car requires careful consideration of ethical principles, societal norms, and the potential implications of the decision made by the vehicle. Here are some steps to approach the resolution:\n",
      "\n",
      "1. **Identify the Stakeholders**: Recognize who is affected by the decision—drivers, passengers, pedestrians, and the broader society. Each group may have different perspectives on what is morally acceptable.\n",
      "\n",
      "2. **Evaluate Ethical Frameworks**: Consider different ethical theories to guide the decision:\n",
      "   - **Utilitarianism**: This perspective suggests that the decision should aim for the greatest overall good, which might mean minimizing harm to the most people.\n",
      "   - **Deontological Ethics**: This framework emphasizes adherence to rules and duties; for example, it may suggest that a car should not actively choose to harm any individual, regardless of the number of lives at stake.\n",
      "   - **Virtue Ethics**: Concentrates on the character and qualities one should embody. This might suggest looking at what a virtuous actor would do in a similar situation.\n",
      "\n",
      "3. **Legal and Regulatory Considerations**: Real-world implications should also be reviewed, including existing laws regarding liability and responsibility in accidents involving autonomous vehicles. \n",
      "\n",
      "4. **Algorithmic Decision-Making**: Consider how such situations could be programmed into vehicle algorithms. This involves complex data analysis and decision-making protocols that account for a range of outcomes, emphasizing safety and adherence to ethical standards.\n",
      "\n",
      "5. **Public Discourse**: Engage in discussions with ethicists, policymakers, and the public to gauge different opinions. Building consensus on ethical frameworks can guide the programming of self-driving technology.\n",
      "\n",
      "6. **Reflect on Scenarios**: Create hypothetical scenarios to explore potential outcomes, looking at various perspectives and the trade-offs involved. \n",
      "\n",
      "7. **Long-term Considerations**: Finally, think about the long-term implications of the decision-making process for future advancements in autonomous vehicle technology and society’s acceptance or rejection of such technology based on ethical grounds.\n",
      "\n",
      "Taking these steps can help to systematically approach the moral dilemma posed by self-driving cars in critical situations, ensuring the process is informed, inclusive, and ethically aligned with societal values.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "This is the classic \"trolley problem\" re-imagined for the age of autonomous vehicles, and it's a deeply complex moral dilemma with no easy answer. My approach would involve considering the following interconnected aspects:\n",
      "\n",
      "**1. Utilitarianism vs. Deontology:**\n",
      "\n",
      "*   **Utilitarianism (Greatest Good for the Greatest Number):** A purely utilitarian approach would prioritize minimizing harm, likely choosing to sacrifice the single person to save multiple pedestrians. This aligns with the idea of maximizing overall well-being. However, it can be seen as inherently unjust to the individual sacrificed.\n",
      "*   **Deontology (Moral Duty and Rules):** A deontological approach focuses on adherence to moral rules and principles, regardless of the consequences. This might mean prioritizing the safety of the car's occupants (assuming the single person is inside the car) or adhering to a principle like \"do no harm.\" However, this could result in the car knowingly causing more harm than necessary.\n",
      "\n",
      "**2. Pre-programmed Algorithms vs. Dynamic Decision-Making:**\n",
      "\n",
      "*   **Pre-programmed Algorithms:** Establishing fixed rules beforehand, based on different scenarios, is appealing for its predictability and transparency.  These rules could be based on utilitarian or deontological principles, or a hybrid of both. The challenge lies in anticipating all possible scenarios and defining the specific thresholds (e.g., number of pedestrians vs. occupant safety) that trigger different actions.  This requires public debate and ethical consensus.\n",
      "*   **Dynamic Decision-Making (AI-driven):** Allowing the car to dynamically analyze the situation and make a \"best\" choice in real-time could lead to more nuanced decisions. This could involve factors like the age and vulnerability of potential victims, the likelihood of survival in each scenario, and the potential long-term consequences. However, this approach raises concerns about accountability, bias in the AI algorithm, and the perceived lack of control.  Furthermore, explaining why the AI made a specific decision would be extremely difficult.\n",
      "\n",
      "**3. Transparency and Accountability:**\n",
      "\n",
      "*   **Public Debate and Ethical Frameworks:**  A broad public discussion involving ethicists, policymakers, engineers, and the public is crucial to establish ethical guidelines and legal frameworks for autonomous vehicle behavior in these situations. This includes defining the values that should guide these algorithms and establishing clear lines of responsibility in case of accidents.\n",
      "*   **Transparency in Algorithm Design:**  The principles and parameters of the algorithms that govern these decisions should be transparent and accessible to the public. This allows for scrutiny and potential improvements based on feedback and evolving societal values.  However, the precise implementation might need to remain proprietary for security reasons.\n",
      "*   **Post-Accident Analysis:**  Thorough investigations of accidents involving autonomous vehicles should be conducted to understand the reasoning behind the car's decisions and to identify areas for improvement in the algorithms and safety systems.\n",
      "\n",
      "**4. Prioritizing Accident Prevention:**\n",
      "\n",
      "*   **Investing in Safety Technology:**  The primary focus should be on developing and implementing technologies that prevent these dilemmas from arising in the first place. This includes advanced sensor systems, redundant braking systems, sophisticated collision avoidance algorithms, and improved road infrastructure.\n",
      "*   **Predictive Modeling and Risk Mitigation:**  Autonomous vehicle algorithms should incorporate predictive modeling to anticipate potential hazards and proactively adjust speed and trajectory to avoid dangerous situations.\n",
      "*   **Continuous Learning and Improvement:**  Autonomous vehicles should continuously learn from their experiences and update their algorithms to improve safety performance over time.\n",
      "\n",
      "**5. Legal and Regulatory Framework:**\n",
      "\n",
      "*   **Liability and Responsibility:**  Clear legal frameworks are needed to define liability in the event of an accident involving an autonomous vehicle. This includes determining who is responsible (e.g., the manufacturer, the owner, the programmer) and establishing mechanisms for compensation to victims.\n",
      "*   **Data Privacy and Security:**  Autonomous vehicles generate vast amounts of data, and it's crucial to ensure that this data is protected from unauthorized access and misuse.  Privacy policies must be transparent and comply with relevant regulations.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "There is no perfect solution to this moral dilemma. My approach would emphasize a multi-faceted strategy that combines:\n",
      "\n",
      "*   **Ethical deliberation:** involving diverse stakeholders to establish guiding principles.\n",
      "*   **Technological innovation:** focusing on preventing accidents and minimizing harm.\n",
      "*   **Transparent and accountable algorithms:** allowing for public scrutiny and continuous improvement.\n",
      "*   **Robust legal and regulatory frameworks:** defining liability and ensuring data privacy.\n",
      "\n",
      "The goal is not to find a \"right\" answer, but to create a system that is as safe, fair, and transparent as possible, given the inherent limitations of technology and the complexity of human morality. It's an ongoing process that requires constant re-evaluation and adaptation as technology advances and societal values evolve.\n",
      "\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Resolving a moral dilemma involving a self-driving car requires careful consideration of the circumstances, weighing the potential consequences of each option, and considering ethical principles and existing laws. Here's a step-by-step approach to address this challenge:\n",
      "\n",
      "1.  **Understand the context**: Gather all available information about the situation, including the road conditions, weather, vehicle data, and any relevant sensor readings.\n",
      "2.  Identify options: Enumerate potential courses of action the car can take:\n",
      "    *   Harm one person (e.g., a child in the foreground) vs.\n",
      "    *   Harm multiple pedestrians (a group of adults in the background).\n",
      "3.  **Weigh the moral implications**: Assess the severity and nature of each option's harm, considering factors such as the individuals' age, health status, and potential consequences for their families and loved ones.\n",
      "4.  Consider ethical principles: Involve well-defined principles that guide decision-making:\n",
      "    *   The \"Utilitarian\" principle: Minimizing overall suffering takes priority over individual well-being.\n",
      "    *   The \"Kantian\" principle: Respect the inherent worth and dignity of every human life would be a core consideration.\n",
      "5.  Evaluate legal considerations: Determine if there are any relevant laws or regulations that directly apply to this scenario:\n",
      "    *   Laws governing autonomous vehicles, pedestrian safety, and traffic rules.\n",
      "6.  Consult with moral experts (if necessary):\n",
      "    *   In some cases, consulting philosophers, ethicists, or other subject-matter experts can provide valuable insights and guidance on complex dilemmas like these.\n",
      "7.  Select a course of action: Based on the analysis, choose a route that aligns with your values, principles, and legal requirements:\n",
      "    *   Choose an option that minimizes harm while respecting the dignity and worth of all individuals involved.\n",
      "8.  Justify and explain: Provide clear reasoning to support the chosen path and address potential concerns or criticisms.\n",
      "\n",
      "The goal is to make a fair decision in accordance with established laws, ethical principles, and societal values.\n",
      "\n",
      "It's essential to note that resolving moral dilemmas involving autonomous vehicles requires ongoing research, discussion, and evolution of our framework for making decisions.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "<think>\n",
      "First, the user is asking about a classic trolley problem scenario but applied to a self-driving car. In this case, the car has to choose between two outcomes: causing harm to one person (e.g., by swerving and hitting that person) or harming multiple people (e.g., not swerving and allowing the pedestrian group to be hit).\n",
      "\n",
      "I need to approach this resolution without being too prescriptive because it's a moral dilemma. There are no easy answers, and different ethical frameworks can lead to different conclusions.\n",
      "\n",
      "Key elements from classical ethics: I should draw on utilitarianism, deontology, virtue ethics, etc., but keep it balanced. Also, consider the context—what defines a person or pedestrian? It could be adults vs. children, for example, which changes the stakes.\n",
      "\n",
      "Context matters because people aren't identical. But in this scenario, since both options involve harming living beings, I need to assume some default (like avoiding greater harm).\n",
      "\n",
      "The user said \"choose between harming one person or multiple pedestrians.\" It might imply that these are different types of victims—e.g., a known individual vs. strangers. In ethics, we often have problems with the distinction.\n",
      "\n",
      "Break down my approach:\n",
      "\n",
      "1. **Acknowledge the dilemma:** Explain that this is an ethical issue where absolute morality intersects with programming choices. Use frameworks like the trolley problem to show its parallels.\n",
      "\n",
      "2. **Explain key concepts in AI ethics for self-driving cars:**\n",
      "   - Intent vs. outcome: Some argue about intention (e.g., does it matter who we try to save?) versus net utility or harm reduction.\n",
      "   - Programming constraints: The car should minimize global risk, perhaps with pre-defined weights based on societal consensus.\n",
      "\n",
      "3. **Potential resolutions and ethical considerations:**\n",
      "   - Utilitarianism-based: Minimize overall harm; choose the option that saves more lives (e.g., hit one person if it results in fewer deaths or less severe outcomes).\n",
      "   - Deontology-based: Focus on rules, like not causing harm to anyone regardless of circumstance.\n",
      "   - Other heuristics: Age, size, identity. For instance, does knowing a passenger versus an outsider affect responsibility?\n",
      "     - Also, social equity aspects.\n",
      "\n",
      "4. **Practical steps for AI systems:** Since this is about self-driving cars, discuss how engineers might code such decisions or avoid them entirely.\n",
      "   - Value alignment: How society would \"align the value system\" of AI to handle these scenarios ethically without being reduced to binary algorithmic choices?\n",
      "   - Uncertainty and split-second decision requirements.\n",
      "\n",
      "Structure my response:\n",
      "- Introduction: Define the situation briefly.\n",
      "- Ethical frameworks: Discuss 1-2 key theories that could shed light on this.\n",
      "- Potential resolutions: Offer a reasoned approach, such as preferring fewer lives lost, but admit it's complex with factors like certain death versus potential harm etcetera—this is important because in reality there might be nuances I'm missing. For example, if swerving would definitely kill one person while not swerving could have ambiguous outcomes or partial damage to property only.\n",
      "- Caveats: Stress that this isn't the real decision since AIs don't have emotions; it's about programming based on values agreed upon by society and AI researchers.\n",
      "\n",
      "AI ethics resources: I can reference ethical principles like those from IEEE, UN Human Rights Council, etc., which are starting to guide such decisions. For instance, they emphasize safety over maximizing outcomes unless necessary.\n",
      "\n",
      "My response should be professional, empathetic, clear, and concise.\n",
      "</think>\n",
      "This is a deeply challenging question that touches on one of the most famous thought experiments in ethics—the \"trolley problem\"—but applied specifically to self-driving cars. I'll approach this step by step, drawing from ethical principles as well as engineering considerations.\n",
      "\n",
      "To start, it's important to note that real-world engineers strive to avoid programming AI systems with explicit dilemmas or binary choices like \"one vs. multiple lives\" because split-second decisions require handling ambiguity and should leverage data, sensors, and predictive modeling to mitigate extreme outcomes whenever possible. That said, in the idealized thought experiment where such a choice is forced (e.g., due to unavoidable collision scenarios), we need to consider ethical frameworks and societal values.\n",
      "\n",
      "### Step 1: Recognize the dilemma\n",
      "The core issue here involves balancing competing moral principles:\n",
      "- **Minimizing harm**: This could mean reducing overall death tolls or injuries.\n",
      "- **Justice and fairness**: Who is responsible for whose safety? Is the passenger at fault (e.g., in an intersection) while pedestrians are not?\n",
      "- **Other values**: Such as protecting children, upholding societal rules about road use, or avoiding actions that treat people unequally.\n",
      "\n",
      "The decision isn't purely numerical; it involves context. For instance:\n",
      "- Are we weighing a driver vs. pedestrian scenario where the car is at fault compared to an unavoidable crash due to external factors?\n",
      "- Does intent matter (e.g., if pedestrians cross unexpectedly versus they are in your path with no warning)?\n",
      "- Aspects like property loss or potential for greater societal harm can influence, but human life should be the central concern.\n",
      "\n",
      "### Step 2: Frame this through ethical principles\n",
      "Several common philosophical frameworks inform these discussions:\n",
      "\n",
      "The most prevalent approach is **utilitarianism**, which focuses on maximizing good outcomes. This might involve choosing to hit one person if your sensors determine that doing so has a higher probability of saving multiple others than swerving, or if the pedestrians' group includes someone who would cause more global harm (like by triggering an event in their vehicle). For example:\n",
      "- If harming one results in fewer total deaths overall, utilitarianism might suggest this choice.\n",
      "- Utilitarian considerations: You'd also factor in other elements like minimizing severity or urgency—e.g., a swift death vs. long-term hospitalization.\n",
      "\n",
      "Conversely, **deontology** emphasizes rules and duties rather than consequences. For instance:\n",
      "- Many engineers argue that the car should be designed to avoid harm entirely under all possible interpretations (e.g., only swerve if doing so doesn't cause anyone else harm).\n",
      "- Other deontologists might use a \"never sacrifice one for many\" rule, based on human dignity or consent principles.\n",
      "\n",
      "Other ethics-based approaches could include:\n",
      "- **Virtue ethics**: Focusing on the character of the AI and how it aligns with virtues like compassion or responsibility.\n",
      "  - In this view, you'd ask: What would we want from our driving partners? A loyal one that reduces accidents but is impartial among victims?\n",
      "- **Duty-based reasoning**: Prioritizing not to actively cause harm (as per Immanuel Kant), even if it leads to a worse overall outcome.\n",
      "\n",
      "### Step 3: Define what the AI should do\n",
      "Based on these principles, programmers would typically aim for:\n",
      "- A \"preference\" mechanism where fewer lives lost are prioritized. For example, in software specifications, engineers might model this as an optimization algorithm that considers factors like the number of people involved directly versus indirectly (e.g., through risk exposure).\n",
      "- Avoiding harm to specific groups: Pedestrians might be weighted differently due to their role on public roads—self-driving cars could be designed to avoid running over pedestrians unless necessary for self-preservation, but this isn't a direct solution.\n",
      "\n",
      "For the split-second decision:\n",
      "1. **Programming requirements**: AI systems might use machine learning models trained on vast datasets from simulations or real-world scenarios. This helps build predictive capabilities without explicit rules.\n",
      "2. **Fallback considerations**: If a crash is inevitable (e.g., due to sensor error, system lag), safety protocols could kick in based on minimizing the likelihood of multiple threats.\n",
      "\n",
      "### Step 4: Address potential outcomes\n",
      "In discussions like this one:\n",
      "- You might need to consider factors beyond direct comparison, such as \"imminence\" or the possibility that not swerving leads to an uncertain outcome—e.g., some pedestrians could survive with no AI choice if the collision causes a gridlock failure and they scatter before impact.\n",
      "- **The principle of utility in ethics**: If hitting one person avoids multiple deaths (or high-severity injuries), it might be acceptable. But many argue this introduces bias—you are value-laden for human input—but that's inherent to decision-making, not about AIs being conscious.\n",
      "\n",
      "### Final thoughts: Is this the right approach?\n",
      "Most experts recommend against codifying universal moral rules because they can't account for all cases—e.g., what if the one person is guilty of something? The AI should probably be designed proactively: with obstacle avoidance systems that prevent these situations from even needing considered. However, in preparation:\n",
      "- Organizations like IEEE or MIT's Moral Machine project discuss how to set up ethical guidelines.\n",
      "  Example: Some suggest \"programming for safety,\" where the goal is always to save lives without preferenceing certain groups.\n",
      "\n",
      "This isn't just about self-driving cars; it sparks broader debates on ethics and technology. In practice, engineers might build systems with safeguards—like ensuring that splits-second decisions are only activated in truly unavoidable scenarios following exhaustive sensor checks—to keep AI impartial among human risks unless societal laws or values dictate otherwise (e.g., hit-and-run legal rules).\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"2\", \"4\", \"1\", \"3\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.0-flash\n",
      "Rank 2: deepseek-r1\n",
      "Rank 3: gpt-4o-mini\n",
      "Rank 4: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
